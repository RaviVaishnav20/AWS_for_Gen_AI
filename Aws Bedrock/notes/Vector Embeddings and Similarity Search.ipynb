{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ededf4",
   "metadata": {},
   "source": [
    "# Vector Embeddings and Similarity Search\n",
    "\n",
    "## Key Concepts <span style=\"color:blue\">#keyConcepts</span>\n",
    "\n",
    "### Vectors <span style=\"color:green\">#vectors</span>\n",
    "\n",
    "Vectors are mathematical representations of words, sentences, or documents as lists of numbers in multi-dimensional space.\n",
    "\n",
    "- **Definition**: A vector is an ordered list of numbers representing features or dimensions of an object or concept.\n",
    "- **Dimensionality**: Real-world applications often use hundreds or thousands of dimensions.\n",
    "- **Example**: In a simplified 2D space, \"apple\" could be represented as [1, 4], where:\n",
    "  1. Is it a fruit? 1 (yes)\n",
    "  2. Cost: $4\n",
    "\n",
    "This allows complex objects or concepts to be represented numerically, enabling mathematical operations and comparisons.\n",
    "\n",
    "<img src=\"vectors.png\" alt=\"Vector Representation\" width=\"700\" height=\"300\"/>\n",
    "<img src=\"vectors2.png\" alt=\"Vector Representation\" width=\"700\" height=\"300\"/>\n",
    "<img src=\"vectors3.png\" alt=\"Vector Representation\" width=\"700\" height=\"300\"/>\n",
    "\n",
    "### Embedding <span style=\"color:green\">#embedding</span>\n",
    "\n",
    "Embedding is the process of converting text or other data into vector representations using machine learning models.\n",
    "\n",
    "- **Purpose**: To create dense, low-dimensional vector representations of high-dimensional data.\n",
    "- **Models**: Examples include Word2Vec, GloVe, and Amazon Titan embeddings.\n",
    "- **Features**:\n",
    "  1. Considers meaning and context, not just keywords\n",
    "  2. Can create word, sentence, and document embeddings\n",
    "  3. Enables semantic similarity comparisons\n",
    "\n",
    "- **Process**:\n",
    "  1. Input text is tokenized\n",
    "  2. Tokens are passed through the embedding model\n",
    "  3. Model outputs a fixed-length vector for each input\n",
    "\n",
    "<img src=\"embeddings.png\" alt=\"Embedding Process\" width=\"700\" height=\"300\"/>\n",
    "<img src=\"embeddings2.png\" alt=\"Embedding Process\" width=\"700\" height=\"300\"/>\n",
    "\n",
    "### Data Chunking <span style=\"color:green\">#dataChunking</span>\n",
    "\n",
    "Data chunking involves breaking large documents or datasets into smaller, manageable parts before vectorization.\n",
    "\n",
    "- **Purpose**: To improve processing efficiency and maintain context in large documents.\n",
    "- **Methods**:\n",
    "  1. Split by fixed number of characters\n",
    "  2. Split by tokens (words or subwords)\n",
    "  3. Split by paragraphs or sentences\n",
    "- **Benefits**:\n",
    "  1. Enables processing of documents too large for direct embedding\n",
    "  2. Preserves local context within chunks\n",
    "  3. Allows for more granular similarity searches\n",
    "\n",
    "<img src=\"data_chunking.png\" alt=\"Data Chunking\" width=\"700\" height=\"300\"/>\n",
    "\n",
    "### Vector Store <span style=\"color:green\">#vectorStore</span>\n",
    "\n",
    "A vector store is a specialized database designed for efficient storage and retrieval of vector representations.\n",
    "\n",
    "- **Purpose**: To store and index large numbers of vector embeddings for fast similarity search.\n",
    "- **Examples**:\n",
    "  - Pinecone\n",
    "  - Chroma DB\n",
    "  - Facebook AI Similarity Search (FAISS)\n",
    "- **Features**:\n",
    "  1. Optimized for high-dimensional vector data\n",
    "  2. Supports efficient similarity search algorithms\n",
    "  3. Scales to millions or billions of vectors\n",
    "  4. Often supports metadata storage alongside vectors\n",
    "\n",
    "<img src=\"vector_store.png\" alt=\"Vector Store\" width=\"700\" height=\"300\"/>\n",
    "\n",
    "### Cosine Similarity <span style=\"color:green\">#cosineSimilarity</span>\n",
    "\n",
    "Cosine similarity is a measure of similarity between two vectors based on the cosine of the angle between them.\n",
    "\n",
    "- **Formula**: cos(θ) = (A · B) / (||A|| ||B||)\n",
    "  Where A and B are vectors, · is the dot product, and ||A|| is the magnitude of A.\n",
    "- **Range**: Values from -1 to 1\n",
    "  - 1: Vectors are identical (0° angle)\n",
    "  - 0: Vectors are orthogonal (90° angle)\n",
    "  - -1: Vectors are opposite (180° angle)\n",
    "- **Usage**: Commonly used in information retrieval and text similarity tasks\n",
    "- **Advantage**: Measures orientation rather than magnitude, making it useful for high-dimensional spaces\n",
    "\n",
    "<img src=\"cosine_similarity.png\" alt=\"Cosine Similarity\" width=\"700\" height=\"300\"/>\n",
    "\n",
    "### K-Nearest Neighbors (KNN) <span style=\"color:green\">#KNN</span>\n",
    "\n",
    "KNN is an algorithm used for classification and regression, often applied in similarity search tasks.\n",
    "\n",
    "- **Purpose**: To find the k most similar items to a query item in a dataset.\n",
    "- **Process**:\n",
    "  1. Calculate distance/similarity between query vector and all vectors in the dataset\n",
    "  2. Select the k nearest neighbors based on the calculated distances\n",
    "  3. For classification: majority vote of the neighbors' classes\n",
    "  4. For regression: average of the neighbors' values\n",
    "- **Distance metrics**: Can use various metrics, including Euclidean distance or cosine similarity\n",
    "- **Considerations**:\n",
    "  - Choice of k affects the algorithm's performance and bias-variance tradeoff\n",
    "  - Can be computationally expensive for large datasets\n",
    "  - Approximate Nearest Neighbor (ANN) algorithms are often used for better scalability\n",
    "\n",
    "<img src=\"knn_algorithm.png\" alt=\"KNN Algorithm\" width=\"700\" height=\"300\"/>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "These concepts form the foundation of modern vector-based similarity search systems, enabling efficient and meaningful comparisons of complex data in high-dimensional spaces. They are crucial for applications in natural language processing, recommendation systems, and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e21904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
