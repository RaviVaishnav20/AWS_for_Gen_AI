{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# Building a RAG chatbot with LangChain, Hugging Face, Amazon SageMaker and Amazon OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e03412",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain opensearch-py -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91613d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from transformers import AutoConfig\n",
    "from typing import Dict\n",
    "\n",
    "from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb259e8",
   "metadata": {},
   "source": [
    "## Deploy our LLM on a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd31539",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "\t'SM_NUM_GPUS': '1'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role \n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "    wait=False,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52098837",
   "metadata": {},
   "source": [
    "## Configure the LangChain input and output handlers for our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de541096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            # Mistral prompt, see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "            {\"inputs\": f\"<s>[INST] {prompt} [/INST]\", \"parameters\": {**model_kwargs}}\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        splits = response_json[0][\"generated_text\"].split(\"[/INST] \")\n",
    "        return splits[1]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker') # needed later to check that endpoint is up\n",
    "smrt_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=smrt_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca9201",
   "metadata": {},
   "source": [
    "## Load the Reuters news dataset from the Hugging Face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HuggingFaceDatasetLoader(\"reuters21578\", \n",
    "                                  page_content_column=\"text\", \n",
    "                                  name=\"ModLewis\")\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56623",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "## Split the news articles into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "docs = splitter.split_documents(data)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "## Configure our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model_id)\n",
    "embedding_dimensions = config.hidden_size\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec6feb",
   "metadata": {},
   "source": [
    "## Define credentials for Amazon OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf43ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 's35vkyhnago99udpmal0.us-east-1.aoss.amazonaws.com'\n",
    "index_name = 'julsimon-index-demo'\n",
    "region = 'us-east-1'\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, \"aoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df511e4",
   "metadata": {},
   "source": [
    "## Embed and index chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe581fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_100 = [docs[x:x+100] for x in range(0, len(docs), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab34d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for docs in docs_100:\n",
    "    oss = OpenSearchVectorSearch.from_documents(\n",
    "        docs,\n",
    "        embeddings,\n",
    "        opensearch_url=f'https://{host}:443',\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        index_name=index_name,\n",
    "        timeout=60,\n",
    "    )\n",
    "    print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "## Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = oss.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1db22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "As a helpful news agent, please answer the question using only the context below.\n",
    "If you don't know, say you don't know.\n",
    "Cite the title of the articles you used to build your answer.\n",
    "\n",
    "question: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that our LLM has been deployed\n",
    "\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "## Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64223366",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the worst storms in recent news?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f083366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
